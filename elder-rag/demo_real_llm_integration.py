#!/usr/bin/env python3
"""
ElderDiet RAGç³»ç»Ÿ - çœŸå®å¤§æ¨¡å‹é›†æˆæ¼”ç¤º
æ”¯æŒé’±å¤šå¤šå¹³å°APIï¼Œæä¾›å®Œæ•´çš„æ™ºèƒ½è¥å…»å’¨è¯¢åŠŸèƒ½
"""

import sys
import os
import time
import json
from typing import Dict, Any, Optional, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from services.vector_store import VectorStore
from services.rag_chain import RAGChain, RAGConfig, RAGMode, RAGContext
from services.conversation_manager import ConversationManager
from services.retriever import SearchStrategy


class QianDuoDuoLLMDemo:
    """é’±å¤šå¤šå¹³å°LLMæ¼”ç¤ºç³»ç»Ÿ"""
    
    def __init__(self):
        self.rag_chain = None
        self.conversation_manager = None
        self.api_key = None
        self.available_models = {
            "gpt-4o": {
                "name": "GPT-4o",
                "description": "æœ€æ–°æ¨¡å‹ï¼Œæ€§èƒ½æœ€å¼ºï¼Œæ¨èä½¿ç”¨",
                "cost": "~$0.01-0.03/æ¬¡",
                "speed": "ä¸­ç­‰"
            },
            "gpt-4": {
                "name": "GPT-4", 
                "description": "é«˜è´¨é‡å›ç­”ï¼Œè¾ƒæ…¢",
                "cost": "~$0.05-0.15/æ¬¡",
                "speed": "è¾ƒæ…¢"
            },
            "gpt-3.5-turbo": {
                "name": "GPT-3.5 Turbo",
                "description": "ç»æµå®æƒ ï¼Œå¿«é€Ÿå“åº”",
                "cost": "~$0.005-0.01/æ¬¡", 
                "speed": "å¿«é€Ÿ"
            }
        }
        
    def check_api_setup(self) -> bool:
        """æ£€æŸ¥APIè®¾ç½®"""
        print("ğŸ” æ£€æŸ¥é’±å¤šå¤šå¹³å°APIè®¾ç½®...")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        self.api_key = os.getenv("QIANDUODUO_API_KEY")
        
        if not self.api_key:
            print("âŒ æœªæ‰¾åˆ°API Key")
            print("\nğŸ“‹ è®¾ç½®æ­¥éª¤ï¼š")
            print("1. åœ¨ç»ˆç«¯è¿è¡Œ: export QIANDUODUO_API_KEY='your-api-key'")
            print("2. æˆ–è€…ä¸´æ—¶è®¾ç½®: QIANDUODUO_API_KEY='your-key' python demo_real_llm_integration.py")
            print("3. æˆ–è€…æ°¸ä¹…è®¾ç½®: echo 'export QIANDUODUO_API_KEY=\"your-key\"' >> ~/.bashrc")
            return False
        
        # æ£€æŸ¥openaiåº“
        try:
            import openai
            print("âœ… OpenAIåº“å·²å®‰è£…")
        except ImportError:
            print("âŒ OpenAIåº“æœªå®‰è£…")
            print("è¯·è¿è¡Œ: pip install openai")
            return False
        
        print(f"âœ… API Keyå·²é…ç½® (****{self.api_key[-8:]})")
        return True
    
    def initialize_system(self, model_name: str = "gpt-4o") -> bool:
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        try:
            print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ElderDiet RAGç³»ç»Ÿ (çœŸå®LLMæ¨¡å¼)...")
            print("="*60)
            
            # åŠ è½½å‘é‡å­˜å‚¨
            print("ğŸ“š åŠ è½½çŸ¥è¯†åº“...")
            vector_store = VectorStore(model_name="shibing624/text2vec-base-chinese")
            vector_store.load("data/vector_db")
            
            # é…ç½®çœŸå®LLM
            rag_config = RAGConfig(
                mode=RAGMode.ENHANCED,
                search_strategy=SearchStrategy.HYBRID,
                top_k=3,
                use_few_shot=True,
                enable_quality_check=True,
                response_style="professional",
                max_response_length=1200,
                
                # çœŸå®LLMé…ç½®
                use_real_llm=True,
                llm_provider="openai",
                llm_model=model_name,
                llm_api_key=self.api_key,
                llm_base_url="https://api2.aigcbest.top/v1"  # é’±å¤šå¤šAPIåœ°å€
            )
            
            # åˆå§‹åŒ–ç³»ç»Ÿ
            self.rag_chain = RAGChain(vector_store, rag_config)
            self.conversation_manager = ConversationManager(self.rag_chain)
            
            print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.available_models[model_name]['name']}")
            print(f"ğŸ“Š çŸ¥è¯†åº“æ–‡æ¡£: {vector_store.get_stats()['total_documents']} ä¸ª")
            print(f"ğŸ”— APIåœ°å€: https://api2.aigcbest.top/v1")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def display_main_menu(self):
        """æ˜¾ç¤ºä¸»èœå•"""
        print("\n" + "ğŸŒŸ ElderDiet RAG - çœŸå®LLMé›†æˆæ¼”ç¤º ğŸŒŸ".center(60))
        print("="*60)
        print("ğŸ¤– å½“å‰æ¨¡å¼: çœŸå®å¤§æ¨¡å‹APIè°ƒç”¨")
        print("ğŸ¥ ä¸“ä¸šé¢†åŸŸ: è€å¹´äººè¥å…»è†³é£Ÿå’¨è¯¢")
        print("ğŸ’¡ æŠ€æœ¯æ¶æ„: RAG + Chain-of-Thought + Few-shot")
        print("="*60)
        
        print("\nğŸ“‹ åŠŸèƒ½èœå•:")
        print("  1. ğŸ¯ å•æ¬¡æ™ºèƒ½å’¨è¯¢")
        print("  2. ğŸ’¬ å¤šè½®å¯¹è¯å’¨è¯¢") 
        print("  3. ğŸ”§ æ¨¡å‹æ€§èƒ½æµ‹è¯•")
        print("  4. ğŸ“Š ç³»ç»ŸçŠ¶æ€ç›‘æ§")
        print("  5. ğŸš€ äº¤äº’å¼LLMæµ‹è¯•")
        print("  0. ğŸšª é€€å‡ºç³»ç»Ÿ")
    
    def demo_single_consultation(self):
        """æ¼”ç¤ºå•æ¬¡æ™ºèƒ½å’¨è¯¢"""
        print("\n" + "="*60)
        print("ğŸ¯ å•æ¬¡æ™ºèƒ½å’¨è¯¢æ¼”ç¤º")
        print("="*60)
        print("ğŸ’¡ è¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„å¤§æ¨¡å‹APIï¼Œç”Ÿæˆä¸ªæ€§åŒ–çš„è¥å…»å»ºè®®")
        
        sample_queries = [
            "æˆ‘æ˜¯78å²è€äººï¼Œæœ‰ç³–å°¿ç—…å’Œé«˜è¡€å‹ï¼Œåº”è¯¥æ€ä¹ˆæ§åˆ¶é¥®é£Ÿï¼Ÿ",
            "è€å¹´äººç¼ºé’™ä¸¥é‡ï¼Œé™¤äº†å–ç‰›å¥¶è¿˜æœ‰ä»€ä¹ˆè¡¥é’™æ–¹æ³•ï¼Ÿ",
            "å¿ƒè¡€ç®¡ç–¾ç—…æ‚£è€…çš„é¥®é£Ÿç¦å¿Œæœ‰å“ªäº›ï¼Ÿ",
            "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªé€‚åˆ80å²è€äººçš„ä¸€å‘¨å¥åº·é£Ÿè°±",
            "è€å¹´äººè¥å…»ä¸è‰¯çš„ç—‡çŠ¶å’Œæ”¹å–„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        print("ğŸ¯ æ¨èå’¨è¯¢é—®é¢˜ï¼š")
        for i, query in enumerate(sample_queries, 1):
            print(f"  {i}. {query}")
        
        while True:
            choice = input(f"\nè¯·é€‰æ‹©é—®é¢˜ (1-{len(sample_queries)}) æˆ–è¾“å…¥è‡ªå®šä¹‰é—®é¢˜ (0é€€å‡º): ").strip()
            
            if choice == "0":
                break
            
            query = ""
            if choice.isdigit() and 1 <= int(choice) <= len(sample_queries):
                query = sample_queries[int(choice) - 1]
            else:
                query = choice
            
            if not query:
                continue
            
            # å¤„ç†å’¨è¯¢
            self._process_single_query(query)
    
    def _process_single_query(self, query: str):
        """å¤„ç†å•æ¬¡æŸ¥è¯¢"""
        print(f"\nğŸ’¬ æ‚¨çš„å’¨è¯¢: {query}")
        print("ğŸ¤” AIè¥å…»å¸ˆæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹åˆ†æ...")
        print("â³ è¯·ç¨å€™ï¼ŒçœŸå®APIè°ƒç”¨éœ€è¦å‡ ç§’é’Ÿ...")
        
        start_time = time.time()
        
        try:
            # è°ƒç”¨RAGç³»ç»Ÿ (ä¼šä½¿ç”¨çœŸå®LLM)
            response = self.rag_chain.process_query(query)
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ¤– ä¸“ä¸šè¥å…»å¸ˆå›ç­” (ç”±{self.rag_chain.config.llm_model}ç”Ÿæˆ):")
            print("="*50)
            print(response.answer)
            print("="*50)
            
            # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
            print(f"\nğŸ“Š å›ç­”åˆ†æ:")
            print(f"  ğŸ¯ è¯†åˆ«æ„å›¾: {response.intent.value}")
            print(f"  ğŸ“ˆ ç½®ä¿¡åº¦: {response.confidence_score:.1%}")
            print(f"  ğŸŒŸ è´¨é‡è¯„åˆ†: {response.quality_score:.1f}/100")
            print(f"  â±ï¸ APIè°ƒç”¨è€—æ—¶: {processing_time:.2f}ç§’")
            print(f"  ğŸ“š å‚è€ƒæ–‡æ¡£: {len(response.sources)} ä¸ª")
            print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.rag_chain.config.llm_model}")
            
            # æ˜¾ç¤ºçŸ¥è¯†æ¥æº
            if response.sources:
                print(f"\nğŸ“– çŸ¥è¯†æ¥æº:")
                for i, source in enumerate(response.sources[:3], 1):
                    print(f"  {i}. {source.title} (ç›¸å…³æ€§: {source.relevance_score:.1%})")
            
            # æˆæœ¬ä¼°ç®—
            model_info = self.available_models.get(self.rag_chain.config.llm_model, {})
            if model_info.get("cost"):
                print(f"\nğŸ’° é¢„ä¼°æˆæœ¬: {model_info['cost']}")
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            print("ğŸ”„ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPI Keyè®¾ç½®")
        
        print("\n" + "-"*60)
    
    def demo_conversation_mode(self):
        """æ¼”ç¤ºå¤šè½®å¯¹è¯æ¨¡å¼"""
        print("\n" + "="*60)
        print("ğŸ’¬ å¤šè½®æ™ºèƒ½å¯¹è¯æ¼”ç¤º")
        print("="*60)
        print("ğŸ¯ è¿™æ˜¯çœŸå®LLMé©±åŠ¨çš„å¤šè½®å¯¹è¯ï¼ŒAIä¼šè®°ä½å¯¹è¯å†å²")
        print("ğŸ’¡ è¾“å…¥ 'quit' ç»“æŸå¯¹è¯ï¼Œ'stats' æŸ¥çœ‹ç»Ÿè®¡")
        
        # åˆ›å»ºä¼šè¯
        session_id = self.conversation_manager.create_session(
            user_id="llm_demo_user",
            user_profile={
                "age": 72,
                "gender": "male",
                "conditions": ["diabetes", "hypertension", "high_cholesterol"],
                "preferences": ["æ¸…æ·¡", "ä½ç›", "æ˜“æ¶ˆåŒ–"]
            }
        )
        
        print(f"\nâœ… å·²åˆ›å»ºä¸ªæ€§åŒ–ä¼šè¯")
        print("ğŸ‘´ ç”¨æˆ·æ¡£æ¡ˆ: 72å²ç”·æ€§ï¼Œæ‚£æœ‰ç³–å°¿ç—…ã€é«˜è¡€å‹ã€é«˜è¡€è„‚")
        
        turn_count = 0
        total_cost = 0.0
        
        while True:
            user_input = input(f"\nğŸ’¬ æ‚¨çš„é—®é¢˜ (ç¬¬{turn_count + 1}è½®): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print(f"\nğŸ‘‹ å¯¹è¯ç»“æŸï¼")
                print(f"ğŸ“Š æ€»è®¡ {turn_count} è½®å¯¹è¯")
                print(f"ğŸ’° é¢„ä¼°æ€»æˆæœ¬: ${total_cost:.3f}")
                break
            
            if user_input.lower() == 'stats':
                session_info = self.conversation_manager.get_session_info(session_id)
                print(f"\nğŸ“Š å¯¹è¯ç»Ÿè®¡:")
                print(f"  â±ï¸ å¯¹è¯æ—¶é•¿: {session_info['duration']:.1f}ç§’")
                print(f"  ğŸ’¬ å¯¹è¯è½®æ¬¡: {session_info['total_turns']}")
                print(f"  ğŸŒŸ å¹³å‡è´¨é‡: {session_info['session_stats']['average_quality_score']:.1f}/100")
                print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: ${total_cost:.3f}")
                continue
            
            if not user_input:
                continue
            
            print("ğŸ¤” AIè¥å…»å¸ˆæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹åˆ†æ...")
            print("â³ ç»“åˆå¯¹è¯å†å²ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”...")
            
            try:
                start_time = time.time()
                response, session_info = self.conversation_manager.process_user_input(session_id, user_input)
                processing_time = time.time() - start_time
                
                # æ˜¾ç¤ºå›ç­”
                print(f"\nğŸ¤– AIè¥å…»å¸ˆå›ç­”:")
                print("="*50)
                print(response)  # response æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¯¹è±¡
                print("="*50)
                
                print(f"\nğŸ“Š æœ¬è½®åˆ†æ:")
                print(f"  ğŸŒŸ è´¨é‡è¯„åˆ†: {session_info.get('quality_score', 0):.1f}/100")
                print(f"  â±ï¸ å¤„ç†è€—æ—¶: {session_info.get('processing_time', processing_time):.2f}ç§’")
                print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.rag_chain.config.llm_model}")
                print(f"  ğŸ¯ è¯†åˆ«æ„å›¾: {session_info.get('intent', 'unknown')}")
                print(f"  ğŸ“ˆ ç½®ä¿¡åº¦: {session_info.get('confidence', 0):.1%}")
                
                turn_count += 1
                # ä¼°ç®—æˆæœ¬ (ç²—ç•¥ä¼°ç®—)
                estimated_cost = 0.02 if "gpt-4" in self.rag_chain.config.llm_model else 0.008
                total_cost += estimated_cost
                
            except Exception as e:
                print(f"âŒ å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}")
    
    def demo_model_performance(self):
        """æ¼”ç¤ºæ¨¡å‹æ€§èƒ½æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ”§ æ¨¡å‹æ€§èƒ½æµ‹è¯•")
        print("="*60)
        
        test_queries = [
            "ç³–å°¿ç—…è€äººé¥®é£Ÿæ§åˆ¶è¦ç‚¹",
            "è€å¹´äººè¡¥é’™çš„æœ€ä½³æ–¹æ³•", 
            "é«˜è¡€å‹æ‚£è€…é¥®é£Ÿç¦å¿Œ"
        ]
        
        print("ğŸ¯ å°†ä½¿ç”¨ä»¥ä¸‹æµ‹è¯•é—®é¢˜è¯„ä¼°æ¨¡å‹æ€§èƒ½:")
        for i, query in enumerate(test_queries, 1):
            print(f"  {i}. {query}")
        
        if input("\næ˜¯å¦å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼Ÿ(y/n): ").lower() != 'y':
            return
        
        results = []
        total_time = 0
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}/{len(test_queries)}: {query}")
            print("â³ è°ƒç”¨APIä¸­...")
            
            try:
                start_time = time.time()
                response = self.rag_chain.process_query(query)
                processing_time = time.time() - start_time
                total_time += processing_time
                
                results.append({
                    "query": query,
                    "quality_score": response.quality_score,
                    "confidence_score": response.confidence_score,
                    "processing_time": processing_time,
                    "answer_length": len(response.answer)
                })
                
                print(f"âœ… å®Œæˆ - è´¨é‡: {response.quality_score:.1f}, è€—æ—¶: {processing_time:.2f}s")
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
                results.append({
                    "query": query,
                    "error": str(e)
                })
        
        # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
        print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*50)
        print(f"ğŸ¤– æµ‹è¯•æ¨¡å‹: {self.rag_chain.config.llm_model}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {len([r for r in results if 'error' not in r])}/{len(results)}")
        
        successful_results = [r for r in results if 'error' not in r]
        if successful_results:
            avg_quality = sum(r['quality_score'] for r in successful_results) / len(successful_results)
            avg_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
            avg_length = sum(r['answer_length'] for r in successful_results) / len(successful_results)
            
            print(f"ğŸŒŸ å¹³å‡è´¨é‡: {avg_quality:.1f}/100")
            print(f"â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’")
            print(f"ğŸ“ å¹³å‡é•¿åº¦: {avg_length:.0f}å­—ç¬¦")
            print(f"ğŸ’° é¢„ä¼°æ€»æˆæœ¬: ${total_time * 0.01:.3f}")
    
    def demo_system_monitoring(self):
        """æ¼”ç¤ºç³»ç»ŸçŠ¶æ€ç›‘æ§"""
        print("\n" + "="*60)
        print("ğŸ“Š ç³»ç»ŸçŠ¶æ€ç›‘æ§")
        print("="*60)
        
        # RAGç³»ç»Ÿç»Ÿè®¡
        rag_stats = self.rag_chain.get_stats()
        print("ğŸ”— RAGç³»ç»ŸçŠ¶æ€:")
        print(f"  ğŸ“ˆ å¤„ç†æŸ¥è¯¢æ€»æ•°: {rag_stats['total_queries']}")
        print(f"  âœ… æˆåŠŸå“åº”æ•°: {rag_stats['successful_responses']}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {rag_stats['successful_responses']/max(rag_stats['total_queries'], 1)*100:.1f}%")
        print(f"  ğŸŒŸ å¹³å‡è´¨é‡åˆ†æ•°: {rag_stats['average_quality_score']:.1f}/100")
        print(f"  â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {rag_stats['average_processing_time']:.2f}ç§’")
        
        # LLMé…ç½®ä¿¡æ¯
        config = self.rag_chain.config
        print(f"\nğŸ¤– LLMé…ç½®ä¿¡æ¯:")
        print(f"  ğŸ·ï¸ æä¾›å•†: {config.llm_provider}")
        print(f"  ğŸ¤– æ¨¡å‹: {config.llm_model}")
        print(f"  ğŸ”— APIåœ°å€: {config.llm_base_url}")
        print(f"  ğŸ“ æœ€å¤§é•¿åº¦: {config.max_response_length}")
        print(f"  ğŸ¨ å›ç­”é£æ ¼: {config.response_style}")
        
        # çŸ¥è¯†åº“ä¿¡æ¯
        vector_stats = self.rag_chain.vector_store.get_stats()
        print(f"\nğŸ“š çŸ¥è¯†åº“çŠ¶æ€:")
        print(f"  ğŸ“„ æ–‡æ¡£æ€»æ•°: {vector_stats['total_documents']}")
        print(f"  ğŸ”¢ å‘é‡ç»´åº¦: {vector_stats['vector_dimension']}")
        print(f"  ğŸ·ï¸ æ–‡æ¡£ç±»åˆ«: {', '.join(vector_stats['categories'])}")
        
        # APIè¿æ¥æµ‹è¯•
        print(f"\nğŸ” APIè¿æ¥æµ‹è¯•:")
        try:
            test_response = self.rag_chain.process_query("æµ‹è¯•è¿æ¥")
            print("âœ… APIè¿æ¥æ­£å¸¸")
            print(f"ğŸ“ˆ æµ‹è¯•å“åº”è´¨é‡: {test_response.quality_score:.1f}/100")
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¼‚å¸¸: {str(e)}")
    
    def demo_interactive_llm_test(self):
        """äº¤äº’å¼LLMæµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ äº¤äº’å¼LLMæµ‹è¯•")
        print("="*60)
        print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªè‡ªç”±æµ‹è¯•ç¯å¢ƒï¼Œæ‚¨å¯ä»¥æµ‹è¯•ä¸åŒçš„é—®é¢˜å’Œå‚æ•°")
        
        # é€‰æ‹©æ¨¡å‹
        print("\nğŸ¤– å¯ç”¨æ¨¡å‹:")
        models = list(self.available_models.keys())
        for i, model_key in enumerate(models, 1):
            model = self.available_models[model_key]
            print(f"  {i}. {model['name']} - {model['description']}")
            print(f"     ğŸ’° æˆæœ¬: {model['cost']}, âš¡ é€Ÿåº¦: {model['speed']}")
        
        while True:
            try:
                choice = input(f"\né€‰æ‹©æ¨¡å‹ (1-{len(models)}) æˆ–å›è½¦ä½¿ç”¨å½“å‰æ¨¡å‹: ").strip()
                if choice == "":
                    break
                elif choice.isdigit() and 1 <= int(choice) <= len(models):
                    selected_model = models[int(choice) - 1]
                    # æ›´æ–°é…ç½®
                    new_config = self.rag_chain.config
                    new_config.llm_model = selected_model
                    self.rag_chain.update_config(new_config)
                    print(f"âœ… å·²åˆ‡æ¢åˆ° {self.available_models[selected_model]['name']}")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æ•°å­—")
        
        print(f"\nğŸ¤– å½“å‰ä½¿ç”¨æ¨¡å‹: {self.available_models[self.rag_chain.config.llm_model]['name']}")
        print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡ºæµ‹è¯•ï¼Œ'help' æŸ¥çœ‹å¸®åŠ©")
        
        while True:
            query = input("\nğŸ§ª æµ‹è¯•é—®é¢˜: ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                break
            
            if query.lower() == 'help':
                print("\nâ“ æµ‹è¯•å¸®åŠ©:")
                print("  â€¢ ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡Œæµ‹è¯•")
                print("  â€¢ å¯ä»¥æµ‹è¯•å„ç§è¥å…»ç›¸å…³é—®é¢˜")
                print("  â€¢ ç³»ç»Ÿä¼šæ˜¾ç¤ºè¯¦ç»†çš„åˆ†æç»“æœ")
                print("  â€¢ è¾“å…¥ 'quit' é€€å‡ºæµ‹è¯•")
                continue
            
            if not query:
                continue
            
            # å¤„ç†æµ‹è¯•æŸ¥è¯¢
            self._process_single_query(query)
    
    def run(self):
        """è¿è¡Œæ¼”ç¤ºç¨‹åº"""
        print("ğŸŒŸ ElderDiet RAG - çœŸå®å¤§æ¨¡å‹é›†æˆæ¼”ç¤º ğŸŒŸ")
        print("="*60)
        
        # æ£€æŸ¥APIè®¾ç½®
        if not self.check_api_setup():
            print("\nâŒ APIè®¾ç½®æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆé…ç½®é’±å¤šå¤šå¹³å°API Key")
            return
        
        # é€‰æ‹©æ¨¡å‹
        print("\nğŸ¤– é€‰æ‹©LLMæ¨¡å‹:")
        models = list(self.available_models.keys())
        for i, model_key in enumerate(models, 1):
            model = self.available_models[model_key]
            print(f"  {i}. {model['name']} - {model['description']}")
        
        while True:
            try:
                choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}) [é»˜è®¤: 1]: ").strip()
                if choice == "":
                    choice = "1"
                
                if choice.isdigit() and 1 <= int(choice) <= len(models):
                    selected_model = models[int(choice) - 1]
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æ•°å­—")
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        if not self.initialize_system(selected_model):
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        # ä¸»å¾ªç¯
        while True:
            self.display_main_menu()
            
            choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (0-5): ").strip()
            
            if choice == "0":
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ElderDiet RAGç³»ç»Ÿï¼")
                break
            elif choice == "1":
                self.demo_single_consultation()
            elif choice == "2":
                self.demo_conversation_mode()
            elif choice == "3":
                self.demo_model_performance()
            elif choice == "4":
                self.demo_system_monitoring()
            elif choice == "5":
                self.demo_interactive_llm_test()
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


def main():
    """ä¸»å‡½æ•°"""
    try:
        demo = QianDuoDuoLLMDemo()
        demo.run()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        print("ğŸ”§ å»ºè®®æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    main()