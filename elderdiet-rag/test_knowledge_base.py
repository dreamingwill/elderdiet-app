#!/usr/bin/env python3
"""
çŸ¥è¯†åº“æ„å»ºå’Œæ£€ç´¢æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ•´ä¸ªçŸ¥è¯†åº“æ„å»ºæµç¨‹
"""

import sys
import os
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from utils.text_processor import TextProcessor
from services.vector_store import VectorStore

def load_nutrition_data():
    """åŠ è½½è¥å…»çŸ¥è¯†æ•°æ®"""
    data_path = "src/data/nutrition_knowledge.json"
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return []
    
    with open(data_path, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} æ¡è¥å…»çŸ¥è¯†")
    return documents

def test_text_processing(documents):
    """æµ‹è¯•æ–‡æœ¬å¤„ç†"""
    print("\n" + "="*50)
    print("ğŸ”§ æµ‹è¯•æ–‡æœ¬å¤„ç†æ¨¡å—")
    print("="*50)
    
    processor = TextProcessor()
    
    # å¤„ç†æ–‡æ¡£
    processed_docs = processor.process_documents(documents)
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœç¤ºä¾‹
    if processed_docs:
        sample_doc = processed_docs[0]
        print(f"\nğŸ“„ ç¤ºä¾‹æ–‡æ¡£å¤„ç†ç»“æœ:")
        print(f"æ ‡é¢˜: {sample_doc['title']}")
        print(f"åŸå§‹å†…å®¹: {sample_doc['content'][:100]}...")
        print(f"åˆ†è¯ç»“æœ: {sample_doc['tokens'][:10]}...")
        print(f"æå–å…³é”®è¯: {sample_doc['extracted_keywords']}")
        print(f"å¤„ç†åæ–‡æœ¬: {sample_doc['processed_text'][:100]}...")
    
    return processed_docs

def test_vector_store(processed_docs):
    """æµ‹è¯•å‘é‡å­˜å‚¨"""
    print("\n" + "="*50)
    print("ğŸ—„ï¸  æµ‹è¯•å‘é‡å­˜å‚¨æ¨¡å—")
    print("="*50)
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨å™¨ï¼ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    print("æ­£åœ¨åˆå§‹åŒ–å‘é‡å­˜å‚¨å™¨...")
    vector_store = VectorStore(model_name="shibing624/text2vec-base-chinese")
    
    # æ·»åŠ æ–‡æ¡£
    vector_store.add_documents(processed_docs)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = vector_store.get_stats()
    print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    return vector_store

def test_search(vector_store):
    """æµ‹è¯•æœç´¢åŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ” æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("="*50)
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "ç³–å°¿ç—…è€å¹´äººåº”è¯¥æ€ä¹ˆæ§åˆ¶é¥®é£Ÿï¼Ÿ",
        "é«˜è¡€å‹æ‚£è€…éœ€è¦æ³¨æ„ä»€ä¹ˆè¥å…»ï¼Ÿ",
        "è€å¹´äººç¼ºé’™æ€ä¹ˆåŠï¼Ÿ",
        "å¦‚ä½•æå‡å…ç–«åŠ›ï¼Ÿ",
        "ä¾¿ç§˜çš„è€äººåƒä»€ä¹ˆå¥½ï¼Ÿ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” æŸ¥è¯¢ {i}: {query}")
        results = vector_store.search(query, top_k=3)
        
        for j, result in enumerate(results, 1):
            print(f"  ç»“æœ {j}: {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
            print(f"    ç±»åˆ«: {result.get('category', 'N/A')}")
            print(f"    å†…å®¹æ‘˜è¦: {result['content'][:80]}...")
        
        if not results:
            print("  âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")

def test_save_and_load(vector_store):
    """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ’¾ æµ‹è¯•ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½")
    print("="*50)
    
    save_dir = "data/vector_db"
    
    # ä¿å­˜
    print("æ­£åœ¨ä¿å­˜å‘é‡æ•°æ®åº“...")
    vector_store.save(save_dir)
    
    # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨å™¨å¹¶åŠ è½½
    print("æ­£åœ¨æµ‹è¯•åŠ è½½åŠŸèƒ½...")
    new_vector_store = VectorStore(model_name="shibing624/text2vec-base-chinese")
    new_vector_store.load(save_dir)
    
    # éªŒè¯åŠ è½½æ˜¯å¦æˆåŠŸ
    original_stats = vector_store.get_stats()
    loaded_stats = new_vector_store.get_stats()
    
    print("ğŸ“Š å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸå§‹æ–‡æ¡£æ•°: {original_stats['total_documents']}")
    print(f"  åŠ è½½æ–‡æ¡£æ•°: {loaded_stats['total_documents']}")
    
    if original_stats['total_documents'] == loaded_stats['total_documents']:
        print("âœ… ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ­£å¸¸")
        
        # æµ‹è¯•æœç´¢æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        test_query = "ç³–å°¿ç—…é¥®é£Ÿå»ºè®®"
        results = new_vector_store.search(test_query, top_k=2)
        print(f"\nğŸ” åŠ è½½åæœç´¢æµ‹è¯•: {test_query}")
        for result in results:
            print(f"  {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
    else:
        print("âŒ ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½å¼‚å¸¸")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ElderDiet RAG çŸ¥è¯†åº“æ„å»ºæµ‹è¯•")
    print("="*50)
    
    try:
        # 1. åŠ è½½æ•°æ®
        documents = load_nutrition_data()
        if not documents:
            return
        
        # 2. æµ‹è¯•æ–‡æœ¬å¤„ç†
        processed_docs = test_text_processing(documents)
        
        # 3. æµ‹è¯•å‘é‡å­˜å‚¨
        vector_store = test_vector_store(processed_docs)
        
        # 4. æµ‹è¯•æœç´¢
        test_search(vector_store)
        
        # 5. æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
        test_save_and_load(vector_store)
        
        print("\n" + "="*50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼çŸ¥è¯†åº“æ„å»ºæˆåŠŸ")
        print("="*50)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 