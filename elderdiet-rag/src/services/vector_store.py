"""
åŸºäºFAISSçš„å‘é‡å­˜å‚¨æ¨¡å—
æ”¯æŒæ–‡æ¡£å‘é‡åŒ–ã€å­˜å‚¨ã€æ£€ç´¢åŠŸèƒ½
"""

import json
import numpy as np
import faiss
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
import pickle
import os

class VectorStore:
    """FAISSå‘é‡å­˜å‚¨å™¨"""
    
    def __init__(self, model_name: str = "shibing624/text2vec-base-chinese", dimension: int = 768):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨å™¨
        
        Args:
            model_name: å¥å­å‘é‡æ¨¡å‹åç§°
            dimension: å‘é‡ç»´åº¦
        """
        self.model_name = model_name
        self.dimension = dimension
        
        # åˆå§‹åŒ–å¥å­å‘é‡æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½å¥å­å‘é‡æ¨¡å‹: {model_name}")
        self.encoder = SentenceTransformer(model_name)
        print("âœ… å¥å­å‘é‡æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.documents = []
        self.doc_ids = []
    
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡çŸ©é˜µ
        """
        vectors = self.encoder.encode(texts, show_progress_bar=True)
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºå†…ç§¯ç›¸ä¼¼åº¦ï¼‰
        vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
        
        return vectors.astype('float32')
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£éœ€è¦åŒ…å«processed_textå­—æ®µ
        """
        if not documents:
            return
        
        print(f"æ­£åœ¨å‘é‡åŒ– {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = []
        for doc in documents:
            if 'processed_text' in doc:
                texts.append(doc['processed_text'])
            else:
                # å¦‚æœæ²¡æœ‰processed_textï¼Œä½¿ç”¨title+content
                text = f"{doc.get('title', '')} {doc.get('content', '')}"
                texts.append(text)
        
        # ç¼–ç ä¸ºå‘é‡
        vectors = self.encode_texts(texts)
        
        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(vectors)
        
        # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        for i, doc in enumerate(documents):
            doc_with_id = doc.copy()
            doc_with_id['vector_id'] = len(self.documents) + i
            self.documents.append(doc_with_id)
            self.doc_ids.append(doc.get('id', f'doc_{len(self.documents)}'))
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
        print(f"ğŸ“Š å½“å‰æ•°æ®åº“ä¸­å…±æœ‰ {len(self.documents)} ä¸ªæ–‡æ¡£")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if self.index.ntotal == 0:
            return []
        
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.encode_texts([query])
        
        # æœç´¢ç›¸ä¼¼å‘é‡
        scores, indices = self.index.search(query_vector, min(top_k, self.index.ntotal))
        
        # ç»„è£…ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                result = self.documents[idx].copy()
                result['similarity_score'] = float(score)
                results.append(result)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": len(self.documents),
            "index_size": self.index.ntotal,
            "vector_dimension": self.dimension,
            "model_name": self.model_name
        }
    
    def save(self, save_dir: str):
        """
        ä¿å­˜å‘é‡æ•°æ®åº“
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, os.path.join(save_dir, "faiss_index.idx"))
        
        # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
        with open(os.path.join(save_dir, "documents.json"), 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config = {
            "model_name": self.model_name,
            "dimension": self.dimension,
            "doc_ids": self.doc_ids
        }
        with open(os.path.join(save_dir, "config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_dir}")
    
    def load(self, save_dir: str):
        """
        åŠ è½½å‘é‡æ•°æ®åº“
        
        Args:
            save_dir: ä¿å­˜ç›®å½•
        """
        # åŠ è½½é…ç½®
        config_path = os.path.join(save_dir, "config.json")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # éªŒè¯æ¨¡å‹é…ç½®
        if config["model_name"] != self.model_name:
            print(f"âš ï¸  æ¨¡å‹åç§°ä¸åŒ¹é…: {config['model_name']} != {self.model_name}")
        
        # åŠ è½½FAISSç´¢å¼•
        index_path = os.path.join(save_dir, "faiss_index.idx")
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
        
        # åŠ è½½æ–‡æ¡£å…ƒæ•°æ®
        docs_path = os.path.join(save_dir, "documents.json")
        if os.path.exists(docs_path):
            with open(docs_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
        
        self.doc_ids = config.get("doc_ids", [])
        
        print(f"âœ… å‘é‡æ•°æ®åº“å·²ä» {save_dir} åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")


def test_vector_store():
    """æµ‹è¯•å‘é‡å­˜å‚¨å™¨"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_docs = [
        {
            "id": "1",
            "title": "ç³–å°¿ç—…è¥å…»",
            "content": "ç³–å°¿ç—…æ‚£è€…éœ€è¦æ§åˆ¶è¡€ç³–ï¼Œåˆç†é¥®é£Ÿ",
            "processed_text": "ç³–å°¿ç—… æ‚£è€… æ§åˆ¶ è¡€ç³– åˆç† é¥®é£Ÿ"
        },
        {
            "id": "2", 
            "title": "é«˜è¡€å‹é¥®é£Ÿ",
            "content": "é«˜è¡€å‹æ‚£è€…åº”è¯¥é™åˆ¶ç›åˆ†æ‘„å…¥",
            "processed_text": "é«˜è¡€å‹ æ‚£è€… é™åˆ¶ ç›åˆ† æ‘„å…¥"
        }
    ]
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨å™¨
    print("åˆå§‹åŒ–å‘é‡å­˜å‚¨å™¨...")
    vector_store = VectorStore()
    
    # æ·»åŠ æ–‡æ¡£
    vector_store.add_documents(test_docs)
    
    # æµ‹è¯•æœç´¢
    print("\næµ‹è¯•æœç´¢:")
    query = "ç³–å°¿ç—…æ€ä¹ˆæ§åˆ¶è¡€ç³–"
    results = vector_store.search(query, top_k=2)
    
    print(f"æŸ¥è¯¢: {query}")
    for i, result in enumerate(results):
        print(f"ç»“æœ {i+1}: {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\næ•°æ®åº“ç»Ÿè®¡:")
    stats = vector_store.get_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")

if __name__ == "__main__":
    test_vector_store() 