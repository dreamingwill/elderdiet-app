#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆå¥åº·æ–‡ç« æ•°æ®è„šæœ¬
æ ¹æ®Excelæ•°æ®å’Œå›¾ç‰‡URLï¼Œç”Ÿæˆç¬¦åˆHealthArticleå®ä½“ç±»è¦æ±‚çš„JSONæ ¼å¼æ•°æ®
"""

import pandas as pd
import json
import os
import re
from datetime import datetime, timedelta
from pathlib import Path

def load_image_urls():
    """åŠ è½½å›¾ç‰‡URLæ˜ å°„"""
    url_file = "image_urls.json"
    if os.path.exists(url_file):
        with open(url_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print(f"âš ï¸  å›¾ç‰‡URLæ–‡ä»¶ä¸å­˜åœ¨: {url_file}")
        print("   è¯·å…ˆè¿è¡Œ upload_images_to_oss.py ä¸Šä¼ å›¾ç‰‡")
        return {}

def parse_content_to_paragraphs(content_text, image_id, image_urls):
    """å°†å†…å®¹æ–‡æœ¬è§£æä¸ºæ®µè½ç»“æ„"""
    paragraphs = []
    order = 1
    
    # åˆ†å‰²å†…å®¹ï¼Œå¤„ç†æ¢è¡Œç¬¦
    content_text = content_text.replace('\\n', '\n').replace('/n', '\n')
    sections = re.split(r'\n+', content_text.strip())
    
    # æ·»åŠ æ–‡æœ¬æ®µè½
    for section in sections:
        section = section.strip()
        if section:
            paragraphs.append({
                "type": "text",
                "content": section,
                "order": order
            })
            order += 1
    
    # åœ¨ä¸­é—´ä½ç½®æ’å…¥å›¾ç‰‡
    if image_id in image_urls:
        insert_position = len(paragraphs) // 2 if len(paragraphs) > 1 else 1
        
        image_paragraph = {
            "type": "image",
            "url": image_urls[image_id],
            "caption": f"ç›¸å…³å›¾ç‰‡ - {image_id}",
            "altText": f"å¥åº·æ–‡ç« é…å›¾ {image_id}",
            "order": insert_position + 1
        }
        
        # è°ƒæ•´åç»­æ®µè½çš„order
        for p in paragraphs[insert_position:]:
            p["order"] += 1
        
        paragraphs.insert(insert_position, image_paragraph)
    
    return paragraphs

def parse_category_to_tags(category_str):
    """å°†åˆ†ç±»å­—ç¬¦ä¸²è§£æä¸ºæ ‡ç­¾åˆ—è¡¨"""
    # ç§»é™¤"ç±»åˆ«ï¼š"å‰ç¼€
    category_str = re.sub(r'^ç±»åˆ«ï¼š', '', category_str)
    
    # åˆ†å‰²æ ‡ç­¾
    tags = re.split(r'[,ï¼Œã€]', category_str)
    
    # æ¸…ç†æ ‡ç­¾
    cleaned_tags = []
    for tag in tags:
        tag = tag.strip()
        if tag:
            cleaned_tags.append(tag)
    
    return cleaned_tags

def estimate_read_time(content):
    """ä¼°ç®—é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
    # ä¸­æ–‡å¹³å‡é˜…è¯»é€Ÿåº¦çº¦ä¸º300å­—/åˆ†é’Ÿ
    char_count = len(content)
    read_time = max(1, round(char_count / 300))
    return read_time

def generate_health_articles_data():
    """ç”Ÿæˆå¥åº·æ–‡ç« æ•°æ®"""
    
    # è¯»å–Excelæ•°æ®
    excel_path = "health_article_content/list.xlsx"
    if not os.path.exists(excel_path):
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
        return None
    
    print(f"ğŸ“– è¯»å–Excelæ•°æ®: {excel_path}")
    df = pd.read_excel(excel_path)
    
    # åŠ è½½å›¾ç‰‡URLæ˜ å°„
    print(f"ğŸ–¼ï¸  åŠ è½½å›¾ç‰‡URLæ˜ å°„...")
    image_urls = load_image_urls()
    
    # ç”Ÿæˆæ–‡ç« æ•°æ®
    articles = []
    base_date = datetime(2024, 1, 1)
    
    for index, row in df.iterrows():
        try:
            # è§£æå†…å®¹ä¸ºæ®µè½
            paragraphs = parse_content_to_paragraphs(
                row['content'], 
                row['image_ID'], 
                image_urls
            )
            
            # è§£ææ ‡ç­¾
            tags = parse_category_to_tags(row['category'])
            
            # ä¼°ç®—é˜…è¯»æ—¶é—´
            read_time = estimate_read_time(row['content'])
            
            # åˆ›å»ºæ–‡ç« å¯¹è±¡
            article = {
                "title": row['title'],
                "subtitle": f"æ¥æºï¼š{row['reference']}" if pd.notna(row['reference']) else "",
                "category": tags[0] if tags else "å¥åº·å…»ç”Ÿ",  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºä¸»åˆ†ç±»
                "content": {
                    "paragraphs": paragraphs
                },
                "read_time": read_time,
                "tags": tags,
                "cover_image": image_urls.get(row['image_ID'], ""),
                "status": 1,  # å·²å‘å¸ƒ
                "is_featured": 1 if index < 20 else 0,  # å‰20ç¯‡è®¾ä¸ºæ¨è
                "is_carousel": 1 if index < 5 else 0,   # å‰5ç¯‡è®¾ä¸ºè½®æ’­
                "carousel_order": index + 1 if index < 5 else 0,
                "created_at": base_date + timedelta(days=index),
                "updated_at": base_date + timedelta(days=index)
            }
            
            articles.append(article)
            print(f"âœ… [{index + 1}/{len(df)}] å¤„ç†å®Œæˆ: {row['title']}")
            
        except Exception as e:
            print(f"âŒ [{index + 1}/{len(df)}] å¤„ç†å¤±è´¥: {row['title']} - {str(e)}")
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    output_file = "health_articles_data.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“Š æ•°æ®ç”Ÿæˆç»Ÿè®¡:")
    print(f"   æ€»æ–‡ç« æ•°: {len(articles)}")
    print(f"   æ¨èæ–‡ç« : {sum(1 for a in articles if a['is_featured'] == 1)}")
    print(f"   è½®æ’­æ–‡ç« : {sum(1 for a in articles if a['is_carousel'] == 1)}")
    print(f"   æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    return articles

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç”Ÿæˆå¥åº·æ–‡ç« æ•°æ®...")
    result = generate_health_articles_data()
    
    if result:
        print(f"\nğŸ‰ æ•°æ®ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(result)} ç¯‡æ–‡ç« æ•°æ®")
    else:
        print(f"\nğŸ’¥ æ•°æ®ç”Ÿæˆå¤±è´¥")
